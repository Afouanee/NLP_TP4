{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52815b5d",
   "metadata": {},
   "source": [
    "# NLP Lab4 \n",
    "\n",
    "## Work to do and assessment policy:\n",
    "\n",
    "- The parts of this lab are independent.\n",
    "- You may work by pairs, or alone. However, there is no benefit, nor increase of grade in case you work alone.\n",
    "- Simply fill this notebook and upload it to [mvproxy](https://mvproxy.esiee.fr) no later than december 21st 23:59\n",
    "\n",
    "Please first indicate below who you have worked with :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eebfd5e-bf5d-4111-9d9c-aefda27037b6",
   "metadata": {},
   "source": [
    "Afouane MOUHAMAD / Axel BLOIS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc2ad6-4365-42aa-95aa-10a005b5d042",
   "metadata": {},
   "source": [
    "## Part A : Recurrent networks ($\\thickapprox$ 8 pts)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In [TD3](https://perso.esiee.fr/~hilairex/5I-SI5/TD3-corr.pdf), we saw that a linear Elman network, used in teacher forcing mode, could properly learn the time series $x_t = \\exp(at)$. This section only intends to demonstrate how Keras' [SimpleRNN](https://keras.io/api/layers/recurrent_layers/simple_rnn/) class operates on this elementary example.\n",
    "\n",
    "Let's say we have a sequence of length 30, which is a realization of $\\exp(at)_{t=1,...,30}$ for some $a \\not= 0$.  We first generate the time series, then compute its image through the SimpleRNN layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec980dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 29, 1), dtype=float32, numpy=\n",
       "array([[[ -1.6612686 ],\n",
       "        [ -0.17471719],\n",
       "        [ -1.8543608 ],\n",
       "        [ -0.38811707],\n",
       "        [ -2.0902045 ],\n",
       "        [ -0.6487644 ],\n",
       "        [ -2.3782642 ],\n",
       "        [ -0.96711993],\n",
       "        [ -2.730101  ],\n",
       "        [ -1.3559604 ],\n",
       "        [ -3.1598358 ],\n",
       "        [ -1.8308911 ],\n",
       "        [ -3.6847148 ],\n",
       "        [ -2.4109726 ],\n",
       "        [ -4.3258038 ],\n",
       "        [ -3.1194854 ],\n",
       "        [ -5.108832  ],\n",
       "        [ -3.9848657 ],\n",
       "        [ -6.065223  ],\n",
       "        [ -5.041843  ],\n",
       "        [ -7.2333636 ],\n",
       "        [ -6.3328385 ],\n",
       "        [ -8.660133  ],\n",
       "        [ -7.9096622 ],\n",
       "        [-10.402794  ],\n",
       "        [ -9.835602  ],\n",
       "        [-12.531284  ],\n",
       "        [-12.187948  ],\n",
       "        [-15.131027  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercice 1 du TD3 : apprendre un Elman sur exp(at)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ParamÃ¨tres\n",
    "a = 0.1  # ParamÃ¨tre de la sÃ©rie\n",
    "sequence_length = 30  # Longueur de la sÃ©quence\n",
    "t = np.arange(sequence_length)\n",
    "x_t = np.exp(a * t)  # SÃ©rie temporelle x_t = exp(a * t)\n",
    "\n",
    "# PrÃ©paration des donnÃ©es\n",
    "X, y = [], []\n",
    "for i in range(len(x_t) - 1):\n",
    "    X.append(x_t[i:i+1])  # Utilisation de la valeur actuelle pour prÃ©dire la suivante\n",
    "    y.append(x_t[i+1])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "hidden= SimpleRNN(1, input_shape=(1, ), activation='linear', return_sequences=True, return_state=False)  # Activation linÃ©aire\n",
    "hidden(X.reshape(1,29,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed6c50",
   "metadata": {},
   "source": [
    "Because we have set <tt>return_sequences=True</tt> in the constructor of <tt>SimpleRNN</tt>, the value of the hidden state is returned at every time step. In addition, the <tt>return_state parameter</tt> controls whether the final state of the hidden state should also be returned or not. Hence, \"return_sequences=True, return_state=False\" corresponds to a many-to-many case, whereas \"return_sequences=False, return_state=False\" to a many-to-one case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18e7f12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-267.0094]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden= SimpleRNN(1, input_shape=(1, ), activation='linear', return_sequences=False, return_state=False)\n",
    "hidden(X.reshape(1,29,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55439f5d",
   "metadata": {},
   "source": [
    "Note that <tt>SimpleRNN</tt> only implements the hidden state equation of Elman's model, not its output equation. To have the complete model, we must also add a Dense layer, which we do in the next snippets. In addition, $h_0$, the initial state, is *not* learnable by SimpleRNN. It can be specified when calling the layer by setting the initial_state parameter to some value or variable -- namely 1.\n",
    "\n",
    "Here is the complete model for the many-to-many case :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c57ea20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m1\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m1\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m1\u001b[0m)          â”‚             \u001b[38;5;34m1\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1</span> (4.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1\u001b[0m (4.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1</span> (4.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1\u001b[0m (4.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 29, 1), dtype=float32, numpy=\n",
       "array([[[  -1.9595227],\n",
       "        [  -3.0423265],\n",
       "        [  -4.23901  ],\n",
       "        [  -5.561549 ],\n",
       "        [  -7.0231814],\n",
       "        [  -8.638535 ],\n",
       "        [ -10.423777 ],\n",
       "        [ -12.396773 ],\n",
       "        [ -14.577273 ],\n",
       "        [ -16.987097 ],\n",
       "        [ -19.650362 ],\n",
       "        [ -22.593729 ],\n",
       "        [ -25.846651 ],\n",
       "        [ -29.441687 ],\n",
       "        [ -33.414814 ],\n",
       "        [ -37.805798 ],\n",
       "        [ -42.65859  ],\n",
       "        [ -48.021755 ],\n",
       "        [ -53.948967 ],\n",
       "        [ -60.499546 ],\n",
       "        [ -67.73905  ],\n",
       "        [ -75.73995  ],\n",
       "        [ -84.58231  ],\n",
       "        [ -94.35463  ],\n",
       "        [-105.15472  ],\n",
       "        [-117.09065  ],\n",
       "        [-130.2819   ],\n",
       "        [-144.86049  ],\n",
       "        [-160.97232  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(29, 1))\n",
    "\n",
    "rnn_layer = SimpleRNN(1, use_bias=False, activation='linear', return_sequences=True, kernel_initializer='ones', recurrent_initializer='identity')\n",
    "\n",
    "# lambda pour crÃ©er dynamiquement h0 selon la taille du lot Ã  traiter\n",
    "x = Lambda(lambda x: rnn_layer(x, initial_state=tf.ones((tf.shape(x)[0], 1))))(inputs)\n",
    "\n",
    "\n",
    "outputs = Dense(1, use_bias=False, activation='linear')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# L'essayer\n",
    "model(X.reshape(1,29,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef63e2e",
   "metadata": {},
   "source": [
    "Notice that there is just one trainable parameter, as expected. Then, we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61771054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "Valeurs rÃ©elles : [ 1.10517092  1.22140276  1.34985881  1.4918247   1.64872127  1.8221188\n",
      "  2.01375271  2.22554093  2.45960311  2.71828183  3.00416602  3.32011692\n",
      "  3.66929667  4.05519997  4.48168907  4.95303242  5.47394739  6.04964746\n",
      "  6.68589444  7.3890561   8.16616991  9.0250135   9.97418245 11.02317638\n",
      " 12.18249396 13.46373804 14.87973172 16.44464677 18.17414537]\n",
      "PrÃ©dictions : [ 0.15435436  0.23964834  0.3339128   0.43809107  0.5532259   0.6804696\n",
      "  0.8210955   0.9765113   1.1482723   1.3380975   1.5478867   1.7797397\n",
      "  2.0359771   2.319163    2.6321318   2.9780161   3.3602772   3.782741\n",
      "  4.249636    4.7656345   5.335901    5.9661427   6.6626678   7.4324474\n",
      "  8.283185    9.223395   10.262489   11.410865   12.680016  ]\n"
     ]
    }
   ],
   "source": [
    "# EntraÃ®nement du modÃ¨le\n",
    "model.fit(X.reshape(1, 29, 1), y.reshape(1,29,1), epochs=2000, verbose=0)\n",
    "\n",
    "# PrÃ©diction\n",
    "predictions = model.predict(X.reshape(1, 29, 1))\n",
    "\n",
    "# Affichage des rÃ©sultats\n",
    "print(\"Valeurs rÃ©elles :\", y)\n",
    "print(\"PrÃ©dictions :\", predictions.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c838b2c-27d2-487a-ab5c-4169ec1a5e18",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697ffaa-4897-4f7e-836c-24314c1a300b",
   "metadata": {},
   "source": [
    "Let $\\Sigma=\\lbrace \\lbrace, \\rbrace, a, b, \\rbrace$ be an alphabet of only 4 characters. We consider strings made from $\\Sigma$ that have the following property : whenever a '{' character appears, it must match a corresponding '}' character later on, without that any new '{' character may appear in the mean time. In other words, our strings obey the regex\n",
    "```pre\n",
    "({[ab]*})*\n",
    "```\n",
    "\n",
    "### Question A1\n",
    "Propose an Elman network that solves this recognition problem *exactly* : it must achieve a 100% accuracy whatever the input source. For your convenience :\n",
    "* it is suggested that $\\lbrace = (1,0,0,0)$, $\\rbrace=(0,1,0,0)$, $a=b=(0,0,1,0)$ as a starter, but <b>you may choose different dimensions and values</b>.\n",
    "* Elman's equations are given as $$ \\begin{align*} h_t &= f(U h_{t-1} + Wx_t) \\\\ y_t&=g(Vh_t) \\end{align*}$$\n",
    "* and dummy expressions of $U,V,W$ are given below as your answer, so that you can use a Latex template.\n",
    "\n",
    "This question does not require any programming at all : to answer, it is sufficient to fully specify all matrices, and ideally add 1-2 sentences exlaining how your network works. Beware : invalid strings must be <tt>rejected</tt>, not accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec61f8-6884-4373-afbe-11d1941a807e",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Your answer here</font>\n",
    "\n",
    "\n",
    "\n",
    "We think that $U=\\left( \\begin{array}{cccc}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 0 & 1 & 2 \\\\\n",
    "3 & 4 & 5 & 6\n",
    "\\end{array}\n",
    "\\right)$, and $W=\\left( \\begin{array}{cccc}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 0 & 1 & 2 \\\\\n",
    "3 & 4 & 5 & 6\n",
    "\\end{array}\n",
    "\\right)$, and $V= (1,2,3,4)^\\top$ are clever choices, because our network would then operate as follows :\n",
    "* if input = '{', then\n",
    "* else if input = '}', then\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1d13a",
   "metadata": {},
   "source": [
    "Nous pensons que \n",
    "$U=\\left( \\begin{array}{c}\n",
    "1\n",
    "\\end{array} \\right)$, \n",
    "et \n",
    "$W=\\left( \\begin{array}{ccc}\n",
    "1 & -1 & 0\n",
    "\\end{array} \\right)$, \n",
    "et \n",
    "$V=\\left( \\begin{array}{c}\n",
    "1\n",
    "\\end{array} \\right)$ \n",
    "sont des choix judicieux, car notre rÃ©seau fonctionnerait alors de la maniÃ¨re suivante :\n",
    "\n",
    "* si lâ€™entrÃ©e est '{', alors $W x_t = +1$ et lâ€™Ã©tat cachÃ© devient $h_t = h_{t-1} + 1$, ce qui signifie quâ€™un bloc est ouvert.\n",
    "* si lâ€™entrÃ©e est '}', alors $W x_t = -1$ et $h_t = h_{t-1} - 1$, ce qui signifie que le bloc ouvert se ferme.\n",
    "* si lâ€™entrÃ©e appartient Ã  {a, b}, alors $W x_t = 0$ et lâ€™Ã©tat cachÃ© ne change pas.\n",
    "* si jamais $h_t < 0$, cela signifie quâ€™un '}' est apparu sans '{' correspondant â†’ la chaÃ®ne doit Ãªtre rejetÃ©e.\n",
    "* si jamais $h_t > 1$, cela signifie quâ€™un second '{' apparaÃ®t avant que le prÃ©cÃ©dent ne soit fermÃ© â†’ les imbrications sont interdites â†’ la chaÃ®ne doit Ãªtre rejetÃ©e.\n",
    "* si la sÃ©quence se termine avec $h_T = 0$, alors la chaÃ®ne est valide ; sinon, elle doit Ãªtre rejetÃ©e.\n",
    "\n",
    "Ainsi, ce rÃ©seau dâ€™Elman reconnaÃ®t exactement le langage $({[ab]^*})^*$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729dcdcc-1076-4db0-b318-fdbcd0421e94",
   "metadata": {},
   "source": [
    "### Question A2\n",
    "\n",
    "The following code trains an LSTM network to properly separate the strings you have just worked on. It should achieve a very high accuracy (nearby 100%). Try to interpret the matrices it prints, in a broad sense : which coefficients are likely to trigger which gate, when which symbol is met ?. If you can't, explain how LSTM could be used to sucessfully process such strings, even if their length become arbitrary large (once again, this amounts to determining which gate should react to which symbol, and why).\n",
    "\n",
    "This question does not require any programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327783c-183d-4286-824f-a8a656b6f083",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cffd85",
   "metadata": {},
   "source": [
    "Dans cette question, on ne cherche pas Ã  analyser prÃ©cisÃ©ment les matrices dâ€™un LSTM.\n",
    "Ce qui compte, câ€™est de comprendre comment un LSTM utilise ses portes pour reconnaÃ®tre un langage rÃ©gulier, par exemple \n",
    "(\n",
    "ğ‘\n",
    "ğ‘\n",
    ")\n",
    "âˆ—\n",
    "(ab)\n",
    "âˆ—\n",
    ".\n",
    "\n",
    "Un LSTM possÃ¨de quatre portes principales :\n",
    "\n",
    "    la porte dâ€™oubli,\n",
    "\n",
    "    la porte dâ€™entrÃ©e,\n",
    "\n",
    "    la porte candidate,\n",
    "\n",
    "    la porte de sortie.\n",
    "\n",
    "On peut comprendre leur rÃ´le de maniÃ¨re intuitive :\n",
    "\n",
    "    La porte dâ€™oubli dÃ©cide si lâ€™on doit garder ou effacer lâ€™information prÃ©cÃ©dente.\n",
    "    Dans notre cas, elle permet dâ€™effacer la mÃ©moire quand un bloc Â« ab Â» vient dâ€™Ãªtre reconnu.\n",
    "\n",
    "    La porte dâ€™entrÃ©e choisit quelles nouvelles informations doivent Ãªtre ajoutÃ©es dans la mÃ©moire.\n",
    "    Par exemple : lorsquâ€™on lit un Â« a Â», on met Ã  jour lâ€™Ã©tat pour signaler que lâ€™on attend un Â« b Â».\n",
    "\n",
    "    La porte candidate propose lâ€™information Ã  stocker (comme â€œun a vient dâ€™Ãªtre vuâ€).\n",
    "\n",
    "    La porte de sortie dÃ©cide quelles informations sont envoyÃ©es vers la sortie, pour suivre correctement lâ€™Ã©tat du langage.\n",
    "\n",
    "GrÃ¢ce Ã  cette organisation, le LSTM est capable de garder une mÃ©moire stable tant que le bloc Â« ab Â» nâ€™est pas terminÃ©, puis de rÃ©initialiser la mÃ©moire exactement au bon moment quand un nouveau bloc commence.\n",
    "\n",
    "Cela lui permet de reconnaÃ®tre correctement des sÃ©quences du type (ğ‘ğ‘)âˆ—(ab)âˆ—, mÃªme si elles sont trÃ¨s longues.\n",
    "Un RNN simple (comme un Elman), lui, a tendance Ã  oublier ou Ã  accumuler des erreurs au fil du temps, ce qui le rend moins fiable pour ce genre de structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "756fc077-59a3-4a4e-834d-3c3a4cf16814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme des donnÃ©es d'entraÃ®nement : X=(4500, 10, 4), y=(4500, 2)\n",
      "\n",
      "DÃ©but de l'entraÃ®nement de l'architecture LSTM...\n",
      "\n",
      "PrÃ©cision sur l'ensemble de test (LSTM): 100.00%\n",
      "\n",
      "================================================================================\n",
      "ANALYSE DES MATRICES DE POIDS D'ENTRÃ‰E LSTM (W_i, W_f, W_c, W_o)\n",
      "Cols: EntrÃ©es ({, }, a, b) / Lignes: Neurones CachÃ©s (h1 Ã  h4)\n",
      "================================================================================\n",
      "\n",
      "--- Matrice d'EntrÃ©e W_i (Input Gate) ---\n",
      "              {        }        a        b\n",
      "h1:  -0.1808   0.1207  -0.3439  -0.4728\n",
      "h2:   1.2246   0.0976   0.5020   0.9941\n",
      "h3:   0.6395   0.5508   0.4928  -0.3733\n",
      "h4:   0.2983  -0.1959  -0.0645  -0.1840\n",
      "\n",
      "--- Matrice d'Oubli W_f (Forget Gate) ---\n",
      "              {        }        a        b\n",
      "h1:   1.8848  -0.2966  -1.1570  -2.1874\n",
      "h2:  -1.3288   0.8784   0.0776  -0.8199\n",
      "h3:  -0.4062   1.0516  -0.1305  -0.8328\n",
      "h4:  -0.7126   0.3533  -0.3774  -1.5801\n",
      "\n",
      "--- Matrice d'Ã‰tat Candidat W_c (Candidate Cell State) ---\n",
      "              {        }        a        b\n",
      "h1:   0.0974   0.8548   0.0722   0.5316\n",
      "h2:   0.6678  -0.4753  -1.4553   0.8038\n",
      "h3:  -0.1639   0.5360  -0.4675  -0.6332\n",
      "h4:   0.1686   0.7332  -0.0989  -0.1649\n",
      "\n",
      "--- Matrice de Sortie W_o (Output Gate) ---\n",
      "              {        }        a        b\n",
      "h1:   0.2497   0.3945   0.2203   0.4637\n",
      "h2:   0.8040   1.2233   0.4050  -2.2271\n",
      "h3:  -0.2162   0.8637  -0.9456   0.3754\n",
      "h4:   0.3390   0.6295   0.4005  -0.1783\n",
      "\n",
      "================================================================================\n",
      "ANALYSE DES MATRICES DE POIDS RÃ‰CURRENTS LSTM (U_i, U_f, U_c, U_o)\n",
      "Cols/Lignes: Neurones CachÃ©s (h1 Ã  h4)\n",
      "================================================================================\n",
      "\n",
      "--- Matrice d'EntrÃ©e U_i ---\n",
      "             h1       h2       h3       h4\n",
      "h1:  -0.1688  -0.4187  -0.2880  -0.8343\n",
      "h2:   0.8505   0.1616   1.1873   0.6925\n",
      "h3:  -0.6498   0.9113  -0.1956   1.0153\n",
      "h4:  -1.5259   0.4902  -1.6588   0.2002\n",
      "\n",
      "--- Matrice d'Oubli U_f ---\n",
      "             h1       h2       h3       h4\n",
      "h1:   0.3414   0.2939   0.6376  -1.1795\n",
      "h2:  -0.0652   0.4688   0.8312  -1.1292\n",
      "h3:  -0.3707  -1.0934   0.0836   0.6433\n",
      "h4:  -0.0723  -0.4702  -0.6820   0.2577\n",
      "\n",
      "--- Matrice d'Ã‰tat Candidat U_c ---\n",
      "             h1       h2       h3       h4\n",
      "h1:   0.7916   0.4780  -0.7606   0.2203\n",
      "h2:   0.3227   0.4884  -0.5217  -1.2500\n",
      "h3:  -0.4659  -0.4367   0.2274  -0.2686\n",
      "h4:  -0.3183   0.3518  -0.2730   1.0015\n",
      "\n",
      "--- Matrice de Sortie U_o ---\n",
      "             h1       h2       h3       h4\n",
      "h1:   0.5359   0.4355   0.2038  -2.0859\n",
      "h2:   0.7221   0.3510   1.5333  -0.7147\n",
      "h3:  -0.5022  -0.9888  -0.1690   2.1314\n",
      "h4:  -1.4740  -0.5649  -2.3979   0.7112\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Fixer la graine pour la reproductibilitÃ© (les rÃ©sultats varient peu, mais c'est une bonne pratique)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# --- 1. ParamÃ¨tres et PrÃ©paration des DonnÃ©es ---\n",
    "\n",
    "CHARS = {'{': 0, '}': 1, 'a': 2, 'b': 3}\n",
    "VOCAB_SIZE = len(CHARS) # 4\n",
    "MAX_LEN = 10\n",
    "# Nous gardons la mÃªme dimension pour l'Ã©tat cachÃ© (h_t et c_t)\n",
    "HIDDEN_DIM = 4 \n",
    "\n",
    "def generate_sequences(num_sequences):\n",
    "    \"\"\"GÃ©nÃ¨re des chaÃ®nes valides (1) et invalides (0) selon ({[ab]*})+.\"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    while len(X_list) < num_sequences:\n",
    "        # --- SÃ©quence Valide (Label 1) ---\n",
    "        valid_seq = \"\"\n",
    "        num_blocks = np.random.randint(1, 4) \n",
    "        \n",
    "        for _ in range(num_blocks):\n",
    "            max_content_len = MAX_LEN - len(valid_seq) - 2\n",
    "            if max_content_len < 0: break\n",
    "            \n",
    "            content_len = np.random.randint(0, max_content_len + 1)\n",
    "            content = ''.join(np.random.choice(['a', 'b'], content_len))\n",
    "            valid_seq += '{' + content + '}'\n",
    "        \n",
    "        if len(valid_seq) > 0 and len(valid_seq) <= MAX_LEN:\n",
    "            X_list.append(valid_seq.ljust(MAX_LEN, ' '))\n",
    "            y_list.append(1) \n",
    "        \n",
    "        if len(X_list) >= num_sequences: break\n",
    "\n",
    "        # --- SÃ©quence Invalide (Label 0) ---\n",
    "        invalid_type = np.random.randint(0, 4)\n",
    "        invalid_seq = \"\"\n",
    "        \n",
    "        if invalid_type == 0:\n",
    "            content_len = np.random.randint(0, 8)\n",
    "            content = ''.join(np.random.choice(['a', 'b'], content_len))\n",
    "            invalid_seq = '{' + content\n",
    "        elif invalid_type == 1:\n",
    "            invalid_seq = '}' + ''.join(np.random.choice(['{', 'a', 'b'], 2))\n",
    "        elif invalid_type == 2:\n",
    "            invalid_seq = 'a{b}'\n",
    "        else:\n",
    "            invalid_seq = '{' + '}' + 'a'\n",
    "            \n",
    "        if len(invalid_seq) > 0 and len(invalid_seq) <= MAX_LEN:\n",
    "            X_list.append(invalid_seq.ljust(MAX_LEN, ' '))\n",
    "            y_list.append(0) \n",
    "\n",
    "        if len(X_list) >= num_sequences: break\n",
    "\n",
    "    return X_list, np.array(y_list)\n",
    "\n",
    "def encode_sequences(sequences):\n",
    "    \"\"\"Encode les chaÃ®nes en tenseurs One-Hot.\"\"\"\n",
    "    X_encoded = np.zeros((len(sequences), MAX_LEN, VOCAB_SIZE), dtype=np.float32)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        for t, char in enumerate(seq):\n",
    "            if char in CHARS:\n",
    "                X_encoded[i, t, CHARS[char]] = 1.0\n",
    "\n",
    "    return X_encoded\n",
    "\n",
    "N_SAMPLES = 5000\n",
    "X_raw, y = generate_sequences(N_SAMPLES)\n",
    "X_data = encode_sequences(X_raw)\n",
    "\n",
    "split_idx = int(0.9 * N_SAMPLES)\n",
    "X_train, X_test = X_data[:split_idx], X_data[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "y_train_oh = to_categorical(y_train, num_classes=2)\n",
    "y_test_oh = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(f\"Forme des donnÃ©es d'entraÃ®nement : X={X_train.shape}, y={y_train_oh.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. Construction et EntraÃ®nement du ModÃ¨le LSTM ---\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(\n",
    "        HIDDEN_DIM, \n",
    "        input_shape=(MAX_LEN, VOCAB_SIZE),\n",
    "        return_sequences=False\n",
    "    ),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nDÃ©but de l'entraÃ®nement de l'architecture LSTM...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train_oh, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test, y_test_oh),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test_oh, verbose=0)\n",
    "print(f\"\\nPrÃ©cision sur l'ensemble de test (LSTM): {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# --- 3. Extraction et Examen des Matrices de Poids ---\n",
    "\n",
    "lstm_layer = model.layers[0]\n",
    "weights = lstm_layer.get_weights()\n",
    "\n",
    "W = weights[0] # Matrice d'entrÃ©e complÃ¨te (Input Kernel W) - Shape (4, 16)\n",
    "U = weights[1] # Matrice rÃ©currente complÃ¨te (Recurrent Kernel U) - Shape (4, 16)\n",
    "# B = weights[2] # Biais (Bias) - Shape (16,)\n",
    "\n",
    "# Keras/TensorFlow stocke les poids dans l'ordre: [i, f, c, o]\n",
    "# Nous devons dÃ©couper W (4x16) et U (4x16) en 4 matrices (4x4)\n",
    "slice_size = HIDDEN_DIM # 4\n",
    "\n",
    "# Slicing pour W (Poids d'EntrÃ©e)\n",
    "W_i = W[:, 0*slice_size:1*slice_size] # Input Gate\n",
    "W_f = W[:, 1*slice_size:2*slice_size] # Forget Gate\n",
    "W_c = W[:, 2*slice_size:3*slice_size] # Candidate Cell State\n",
    "W_o = W[:, 3*slice_size:4*slice_size] # Output Gate\n",
    "\n",
    "# Slicing pour U (Poids RÃ©currents)\n",
    "U_i = U[:, 0*slice_size:1*slice_size]\n",
    "U_f = U[:, 1*slice_size:2*slice_size]\n",
    "U_c = U[:, 2*slice_size:3*slice_size]\n",
    "U_o = U[:, 3*slice_size:4*slice_size]\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSE DES MATRICES DE POIDS D'ENTRÃ‰E LSTM (W_i, W_f, W_c, W_o)\")\n",
    "print(\"Cols: EntrÃ©es ({, }, a, b) / Lignes: Neurones CachÃ©s (h1 Ã  h4)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "char_indices = ['{', '}', 'a', 'b']\n",
    "hidden_indices = ['h1', 'h2', 'h3', 'h4']\n",
    "\n",
    "def print_matrix(name, matrix):\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"{'':>6} {'{':>8} {'}':>8} {'a':>8} {'b':>8}\")\n",
    "    for i in range(HIDDEN_DIM):\n",
    "        print(f\"{hidden_indices[i]}: {matrix[i, 0]:8.4f} {matrix[i, 1]:8.4f} {matrix[i, 2]:8.4f} {matrix[i, 3]:8.4f}\")\n",
    "\n",
    "print_matrix(\"Matrice d'EntrÃ©e W_i (Input Gate)\", W_i)\n",
    "print_matrix(\"Matrice d'Oubli W_f (Forget Gate)\", W_f)\n",
    "print_matrix(\"Matrice d'Ã‰tat Candidat W_c (Candidate Cell State)\", W_c)\n",
    "print_matrix(\"Matrice de Sortie W_o (Output Gate)\", W_o)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSE DES MATRICES DE POIDS RÃ‰CURRENTS LSTM (U_i, U_f, U_c, U_o)\")\n",
    "print(\"Cols/Lignes: Neurones CachÃ©s (h1 Ã  h4)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def print_recurrent_matrix(name, matrix):\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"{'':>6} {'h1':>8} {'h2':>8} {'h3':>8} {'h4':>8}\")\n",
    "    for i in range(HIDDEN_DIM):\n",
    "        print(f\"{hidden_indices[i]}: {matrix[i, 0]:8.4f} {matrix[i, 1]:8.4f} {matrix[i, 2]:8.4f} {matrix[i, 3]:8.4f}\")\n",
    "\n",
    "print_recurrent_matrix(\"Matrice d'EntrÃ©e U_i\", U_i)\n",
    "print_recurrent_matrix(\"Matrice d'Oubli U_f\", U_f)\n",
    "print_recurrent_matrix(\"Matrice d'Ã‰tat Candidat U_c\", U_c)\n",
    "print_recurrent_matrix(\"Matrice de Sortie U_o\", U_o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2ca01-d0be-4171-8d09-2c15142fb57a",
   "metadata": {},
   "source": [
    "## Part B : text classification ($\\thickapprox$ 6 pts)\n",
    "\n",
    "In this part, you will have to achieve the implementation of the RNN-based model shown on slide 28 of [Chapter 4](https://perso.esiee.fr/~hilairex/5I-SI5/rnn.pdf). The network accepts words as input, from sentences which don't exceed a certain length, and aim to perform text classification. \n",
    "\n",
    "You will work on the the IMDB reviews dataset, hosted by Kaggle [here](https://www.kaggle.com/code/trentpark/data-analysis-basics-imdb-dataset). The reviews have two outcomes : positive, or negative. A copy of this dataset can be found at https://mvproxy.esiee.fr/NLP/Lab4\n",
    "\n",
    "The following code snippets perform the first steps on text for you - loading, vectorising, and training a basic (non-recurrent) FFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec980db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "\n",
    "reviews = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a400ba-34c9-47d0-8c80-162a87e9555a",
   "metadata": {},
   "source": [
    "We first perform a standard test/train split. During development, I strongly suggest that you first use a small amount of samples (1000) for validation. IMDb has 50000 reviews, which is too much. Keep in ming that training RNNs is *slow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test= train_test_split(reviews, shuffle=True, train_size=1000, test_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa581b2d-fe3d-4181-bcde-538306d94587",
   "metadata": {},
   "source": [
    "The next step is to vectorize the text. In Lab3, I provided a vecto() function which did this, with relevant padding. I also mentioned Keras offered a TextVectorization layer which did exactly the same job. Its effects are shown below. \n",
    "\n",
    "In particular, note that unknown words yield an index of 1, and 0 is used for padding. So real indexation starts at index 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c57ea19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=int64, numpy=\n",
       "array([[ 8, 10,  2,  5,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  1,  7,  1,  0,  0,  0,  0,  0,  0]])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text vectorization : quick demo\n",
    "vecto= tf.keras.layers.TextVectorization(max_tokens=99, output_mode='int', output_sequence_length=10)\n",
    "vecto.adapt([[\"I am the king of the world\"],[\"You are the queen\"]])\n",
    "vecto([[\"I am the queen\"],[\"World is king unknown\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da1233-51b1-43c4-9b1f-f496d6522edb",
   "metadata": {},
   "source": [
    "We now change the call to adapt the layer to our train data. Note that IMDb reviews are rather long (about 300 words / review on average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61771053",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words=20000  # the vocabulary size\n",
    "seq_len=300     # maximum sequence length\n",
    "vecto= tf.keras.layers.TextVectorization(max_tokens=max_words, output_mode='int', output_sequence_length=300)\n",
    "vecto.adapt(train['review'].to_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006104cd-fa3c-48e6-96df-ccb3a3583e8a",
   "metadata": {},
   "source": [
    "We are now ready to define our model. Below, I first demonstrate a model with input and vectorization layer alone ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f1fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ text_vectorization_1            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             â”‚                        â”‚               â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ text_vectorization_1            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTextVectorization\u001b[0m)             â”‚                        â”‚               â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 11, 260,   2, 614,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building model : vectorization alone\n",
    "model= Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vecto)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.summary()\n",
    "model.predict(tf.constant([['I am the king']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965f05f-1e54-4366-896c-e4ec7f1c4503",
   "metadata": {},
   "source": [
    "As we saw in labs 2 and 3, embeddings are mandatory. Hence, we will add an Embedding layer, but as opposed as what we did before, we will not initialize if from LSA, nor put it constant. Instead, we will let the model optimize this layer, possibly using dropout (if you use the related option). \n",
    "The dimension of 80 below is a crude estimation (barely from lab2 and results on LSA)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f07e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hilairex/pynlp/lib64/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ text_vectorization_1            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)        â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">240,160</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ text_vectorization_1            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTextVectorization\u001b[0m)             â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m80\u001b[0m)        â”‚       \u001b[38;5;34m240,160\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">240,160</span> (938.12 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m240,160\u001b[0m (938.12 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">240,160</span> (938.12 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m240,160\u001b[0m (938.12 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01580559,  0.02355259,  0.02332291, ..., -0.004517  ,\n",
       "          0.0277617 , -0.02131273],\n",
       "        [ 0.01668474, -0.03429715, -0.04509323, ..., -0.00417768,\n",
       "         -0.01822596,  0.0195891 ],\n",
       "        [ 0.03996921, -0.02546388,  0.00366718, ...,  0.02894926,\n",
       "         -0.00926412, -0.01020152],\n",
       "        ...,\n",
       "        [ 0.00812037, -0.04475567,  0.0241028 , ...,  0.04265859,\n",
       "         -0.0465613 , -0.00206579],\n",
       "        [ 0.00812037, -0.04475567,  0.0241028 , ...,  0.04265859,\n",
       "         -0.0465613 , -0.00206579],\n",
       "        [ 0.00812037, -0.04475567,  0.0241028 , ...,  0.04265859,\n",
       "         -0.0465613 , -0.00206579]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vecto)\n",
    "model.add(tf.keras.layers.Embedding(max_words+2, 80, input_length=seq_len))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.summary()\n",
    "model.predict(tf.constant([['I am the king']]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b32f27-592e-481e-a0c1-57bc51735678",
   "metadata": {},
   "source": [
    "### Question B1\n",
    "\n",
    "The following code reuses the previous ingredients up to TextVectorization, but lacks the definition of a model.\n",
    "\n",
    "Fill the part marked as\n",
    "```pre\n",
    "## ....\n",
    "# A vous de jouer: dÃ©finissez ici le modÃ¨le conformÃ©ment au cours\n",
    "## ....\n",
    "```\n",
    "so that the model defined there conform to the one shown on slide 38 of Chapter 4 [here](https://perso.esiee.fr/~hilairex/AIC-5102B/rnn.pdf). To be properly summarized by the summary() function, your model should be built using Keras' *functional* API,  not the *sequential()* function.  The recurrent network should be a bi-directional LSTM. The output network may boil down to a single layer network with appropriate dimensions.\n",
    "\n",
    "Some pieces of advice :\n",
    "- Remember that embedding turns integer indexes into vectors. Hence your input data is a sequence of *vectors* whatever type of RNN you use.\n",
    "- Be careful to overfitting and shapes.\n",
    "- In the end, you want a single scalar to represent a decision : 1 or 0 (positive or negative)\n",
    "- Keras has a [Bidirectional](https://keras.io/api/layers/recurrent_layers/bidirectional/) and a [Concatenate](https://keras.io/api/layers/merging_layers/concatenate/) layers, which can prove useful. However, you may build your model without using them, by resorting to Keras' functional API.\n",
    "\n",
    "With only 5 epochs, your model should achieve a rather good accuracy (~ 85%), but it might take longer to converge, depending on initial conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305646e9-682c-440f-a66b-a36ea7a2c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------------------------\n",
    "# 1. ParamÃ¨tres gÃ©nÃ©raux\n",
    "# -------------------------------\n",
    "\n",
    "max_features = 20000   # Nombre max de mots dans le vocabulaire\n",
    "maxlen = 300           # Longueur max des sÃ©quences (padding/tronquage)\n",
    "batch_size = 32\n",
    "embedding_dim = 128\n",
    "rnn_units = 128\n",
    "epochs = 5\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Chargement du dataset IMDB (fichier CSV)\n",
    "# -------------------------------\n",
    "\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")   # Colonnes : \"review\", \"sentiment\"\n",
    "\n",
    "# Conversion des labels\n",
    "df[\"label\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "\n",
    "texts = df[\"review\"].astype(str).values\n",
    "labels = df[\"label\"].values\n",
    "\n",
    "print(\"Nombre d'exemples :\", len(texts))\n",
    "print(\"Exemple de texte :\", texts[0][:200], \"...\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Split train/test\n",
    "# -------------------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train_text, x_test_text, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. TextVectorization\n",
    "# -------------------------------\n",
    "\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen\n",
    ")\n",
    "\n",
    "# Apprentissage du vocabulaire\n",
    "vectorizer.adapt(x_train_text)\n",
    "\n",
    "# Encodage en sÃ©quences d'entiers\n",
    "x_train = vectorizer(x_train_text)\n",
    "x_test  = vectorizer(x_test_text)\n",
    "\n",
    "print(\"Forme x_train :\", x_train.shape)\n",
    "print(\"Forme x_test  :\", x_test.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Construction du modÃ¨le\n",
    "# -------------------------------\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(maxlen,))\n",
    "\n",
    "## ....\n",
    "# A vous de jouer: dÃ©finissez ici le modÃ¨le conformÃ©ment au cours\n",
    "## ....\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. EntraÃ®nement du modÃ¨le\n",
    "# -------------------------------\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Ã‰valuation sur le jeu de test\n",
    "# -------------------------------\n",
    "\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"\\nPrÃ©cision sur le test : {acc * 100:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a594865-b15e-4fbf-8230-0815143b098d",
   "metadata": {},
   "source": [
    "### Question B2\n",
    "\n",
    "What happens if your code calls SimpleRNN() instead of LSTM() (all other things being equel)? Give a simple explanation of what is very likely to happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce5df72-abae-49d0-a404-0f490929931d",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Your answer here</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0814b90",
   "metadata": {},
   "source": [
    "## Part C : inference in neural machine translation ($\\thickapprox$ 8 pts)\n",
    "\n",
    "In this part, you will work on the coder+decoder network shown on slide 26 of [Chapter 5](https://perso.esiee.fr/~hilairex/AIC-5102B/lstm.pdf) of the course. We will use this model to translate english sentences to french sentences. To keep computation times acceptable on a laptop, we will not perform translation word by word, as this would require vocabularies of about 50k words in each language; but character by character, which involves merely 100 tokens in each language. Shortly said, we are using seq2seq for character sequence translation.\n",
    "\n",
    "The dataset we use for training is derived from transcripts of the European parliament - see https://www.statmt.org/europarl/ \n",
    " \n",
    "On [mvproxy](https://mvproxy.esiee.fr/NLP), you will find several files :\n",
    "- english_filtered and french_filtered : reworked versions of english to french translations of sentences from Europarl.\n",
    "- lab4c_train.py: Python implementation to train the network over the former files. This code is parameterized by lstm_units, which defines the dimension of the hidden space of the LSTM network used in both coder and decoder.\n",
    "- en2fr<i>xxx</i>.keras + voc.pkl : the Keras model produced by training the model with <i>xxx</i> LSTM units, for both coder and decoder  + the related vocabularies found when parsing english_filtered and french_filtered\n",
    "- lab4c_decode.py : simple implementation of greedy decoding from the decoder, still parameterized by lstm_units.\n",
    "\n",
    "Note that we have separated the problem in two parts : \n",
    "- Training, or lab4c_train.py, which produces en2fr<i>xxx</i>.keras + voc.pkl the very first time it is ran. When ran again, it loads the vocabulary and last model it produced from disk, and refines training. If you run it again, the first training epochs should display the training and validation accuracies. All models have been trained using 2000 epochs.\n",
    "- And decoding, which re-reads these files to rebuild the whole model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592a81d-280b-4dce-9d40-06a5ceeb0085",
   "metadata": {},
   "source": [
    "### Question C1\n",
    "\n",
    "Run the decoder for some <i>xxx</i>, and test it using some sentences found in the source (english) file. You should normally find that your text is not properly decoded, but that the output is not completely random, and somewhat resembles some imaginary french language. In other words, the seq2seq models you are using are hallucinating. \n",
    "\n",
    "Assuming that what you did in part A can be generalized, and would be the only way for a RNN to properly distinguish strings (this is really an assumption): \n",
    "- are these hallucinations a surprise ?\n",
    "- what should be changed so that the model you are using have chances not to hallicinate ?\n",
    "  \n",
    "Please provide a clear explanation of what can happen, considering all parameters (in particular, shapes of matrices), and also how decoding is done. You may simplify the problem and explain what happens if the model were an Elman network (extending to LSTM is just a matter of additional gates and matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b84bd-33f6-45d1-99f6-4276a07077c7",
   "metadata": {},
   "source": [
    "### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368f6f7",
   "metadata": {},
   "source": [
    "### Question C2\n",
    "\n",
    "Below is a copy of lab4c_decode.py, the code of the decoder. Change the code of this function so that it no longer uses greedy decoding, but a beam search decoding of depth 4. Your code should be implemented as is Algorithm 1 on slide 30 of [chapter 5](https://perso.esiee.fr/~hilairex/AIC-5102B/lstm.pdf), with k=4. It should output several candidate strings rather than a single one. Test it using 3 strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Robust decoder that:\n",
    " - uses model.inputs[0] to get encoder input tensor\n",
    " - re-applies encoder Embedding before calling encoder LSTM\n",
    " - reuses decoder Embedding + LSTM + Dense for one-step decoding\n",
    " - performs greedy decoding\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "units = 512\n",
    "model_file = f\"en2fr{units}.keras\"\n",
    "voc_file   = \"voc.pkl\"\n",
    "begin = '\\x02'\n",
    "end   = '\\x03'\n",
    "max_target_len = 200\n",
    "\n",
    "# -----------------------\n",
    "# Load model and vocab\n",
    "# -----------------------\n",
    "print(f\"Loading model: {model_file}\")\n",
    "model = keras.models.load_model(model_file)\n",
    "model.summary()\n",
    "\n",
    "with open(voc_file, 'rb') as f:\n",
    "    voc, char2num, num2char = pickle.load(f)\n",
    "print(\"Vocabulary loaded:\", [len(v) for v in voc])\n",
    "\n",
    "\n",
    "# Encoder\n",
    "enc_emb_layer = model.get_layer('l_enc_embedding')\n",
    "enc_lstm_layer = model.get_layer('l_enc_lstm')\n",
    "\n",
    "# Decoder\n",
    "dec_emb_layer = model.get_layer('l_dec_embedding')\n",
    "dec_lstm_layer = model.get_layer('l_dec_lstm')\n",
    "dec_dense_layer = model.get_layer('l_dec_dense')\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Build encoder model: enc_input -> [h, c]\n",
    "# -----------------------\n",
    "# Apply encoder embedding to the encoder input tensor, then call the trained LSTM on it\n",
    "enc_input_tensor = keras.Input(shape=(None,), dtype='int32')\n",
    "enc_embedded = enc_emb_layer(enc_input_tensor)   # (batch, timesteps, emb_dim)\n",
    "enc_out, enc_h, enc_c = enc_lstm_layer(enc_embedded)\n",
    "encoder_model = keras.Model(inputs=enc_input_tensor, outputs=[enc_h, enc_c])\n",
    "print(\"Encoder model built.\")\n",
    "\n",
    "# -----------------------\n",
    "# Build decoder model for one-step decoding\n",
    "# -----------------------\n",
    "state_size = dec_lstm_layer.units\n",
    "\n",
    "dec_state_input_h = keras.Input(shape=(state_size,), name='dec_state_input_h')\n",
    "dec_state_input_c = keras.Input(shape=(state_size,), name='dec_state_input_c')\n",
    "dec_input_single = keras.Input(shape=(1,), dtype='int32', name='dec_input_single')  # a single timestep token\n",
    "\n",
    "# apply decoder embedding then decoder LSTM (one step)\n",
    "dec_embedded = dec_emb_layer(dec_input_single)  # (batch, 1, emb_dim)\n",
    "dec_outputs, dec_h, dec_c = dec_lstm_layer(dec_embedded, initial_state=[dec_state_input_h, dec_state_input_c])\n",
    "dec_outputs = dec_dense_layer(dec_outputs)  # (batch, 1, vocab_size)\n",
    "\n",
    "decoder_model = keras.Model(\n",
    "    [dec_input_single, dec_state_input_h, dec_state_input_c],\n",
    "    [dec_outputs, dec_h, dec_c]\n",
    ")\n",
    "print(\"Decoder model built.\")\n",
    "\n",
    "# -----------------------\n",
    "# Utilities: encode input / greedy decode\n",
    "# -----------------------\n",
    "def encode_input(text: str):\n",
    "    \"\"\"Encode source text into int32 array (1, L). Append end marker if missing.\"\"\"\n",
    "    if not text.endswith(end):\n",
    "        text = text + end\n",
    "    arr = np.zeros((1, len(text)), dtype='int32')\n",
    "    for i, ch in enumerate(text):\n",
    "        arr[0, i] = char2num[0].get(ch, 0)\n",
    "    return arr\n",
    "\n",
    "def decode_greedy(src_text: str, maxlen: int = max_target_len):\n",
    "    enc_seq = encode_input(src_text)\n",
    "    # get encoder states\n",
    "    h, c = encoder_model.predict(enc_seq, verbose=0)\n",
    "\n",
    "    # start token (<begin>)\n",
    "    cur_token = np.array([[char2num[1][begin]]], dtype='int32')\n",
    "    decoded_chars = []\n",
    "\n",
    "    for _ in range(maxlen):\n",
    "        out_tokens, h, c = decoder_model.predict([cur_token, h, c], verbose=0)\n",
    "        probs = out_tokens[0, 0, :]\n",
    "        idx = int(np.argmax(probs))\n",
    "        ch = num2char[1][idx]\n",
    "        if ch == end:\n",
    "            break\n",
    "        decoded_chars.append(ch)\n",
    "        cur_token = np.array([[idx]], dtype='int32')\n",
    "\n",
    "    return \"\".join(decoded_chars)\n",
    "\n",
    "# -----------------------\n",
    "# Interactive loop\n",
    "# -----------------------\n",
    "print(\"Ready. Enter a source sentence (empty to quit).\")\n",
    "try:\n",
    "    while True:\n",
    "        s = input(\"Source > \").strip()\n",
    "        if s == \"\":\n",
    "            break\n",
    "        translation = decode_greedy(s)\n",
    "        print(\"â†’\", translation)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nExiting.\")\n",
    "    sys.exit(0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynlp",
   "language": "python",
   "name": "pynlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
